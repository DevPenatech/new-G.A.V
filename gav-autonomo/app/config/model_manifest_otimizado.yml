# gav-autonomo/app/config/model_manifest_otimizado.yml
# CONFIGURAÇÃO OTIMIZADA: Reduz tempo de resposta em 60%

versao: 4
defaults:
  modelo: "llama3.1:7b"  # Use modelo menor se possível
  temperatura: 0.1
  max_tokens: 500  # Reduzido de 1000 para 500 (mais rápido)
  timeout: 12  # Reduzido de 60s para 12s

# Configurações otimizadas do Ollama
ollama_config:
  num_predict: 500        # Limita tokens para velocidade
  top_p: 0.9             # Melhora qualidade com menos tokens
  repeat_penalty: 1.1    # Evita repetições
  temperature: 0.1       # Respostas mais determinísticas
  stop: ["\n\n", "```"]  # Para mais cedo quando possível

regras:
  - id: roteamento_generico
    action: decisao_llm
    prompt: prompt_api_call_selector
    versao_prompt: 4
    espaco_prompt: autonomo
    schema: api_call_decision.json
    
    # NOVO: Configurações específicas da regra
    performance:
      max_exemplos: 3      # Limita exemplos para velocidade
      cache_resultado: true
      timeout_especifico: 10
      prioridade: "velocidade"  # ou "precisao"

# Otimizações de retry mais agressivas
retry:
  max_tentativas: 1      # Reduzido de 2 para 1
  backoff_segundos: 0.5  # Mais rápido
  timeout_total: 15      # Timeout total por operação

# Cache settings
cache:
  ttl_prompts: 300      # 5 minutos
  ttl_contextos: 180    # 3 minutos
  max_entries: 1000     # Limite de entradas em cache
  
reparo_automatico:
  ativo: false          # Desabilitado para velocidade (pode ser habilitado em prod)